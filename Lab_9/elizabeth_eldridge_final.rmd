---
title: "COMPSCX 415.2 Homework 9/Final Exam"
author: "Elizabeth Eldridge"
date: "March 30, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom)
```

##Exercise 1 - Sampling Distributions, Functions and For Loops (10 points)

#Step 1:
```{r}
samp_fun <- function(samp_size, samp_rate) {
  
  samp_size <- 100 #set the rate parameter
  samp_rate <- 1/10000 #take sample
  our_sample <- rexp(n = samp_size, rate = samp_rate) #take sample
  samp_avg <- our_sample %>% mean() #calculate mean of our sample
  samp_std_dev <- our_sample %>% sd() #calculate standard deviation of our sample
  stats <- list(samp_avg = samp_avg, samp_std_dev = samp_std_dev)
  return(stats)
}

samp_fun(stats)
```
#Step 2:
```{r}
n <- 1000
sample_means <- rep(NA, n) #empty vector for storing sample means
sample_sds <- rep(NA, n) #empty vector for storing sds

for (i in 1:n) {
  
  samp_size <- 50
  samp_rate <- 1/10000
  our_sample <- rexp(n = samp_size, rate = samp_rate)

  sample_means[i] <- mean(our_sample)
  sample_sds[i] <- sd(our_sample)
}

sample_means <- tibble(sample_means)
sample_means
sample_sds <- tibble(sample_sds)
sample_sds
```
#Step 3:
```{r}
#Plot sample means as a histogram
sample_means %>% ggplot(aes(x = sample_means)) +
geom_histogram()

#output standard deviation of sample means
(sd(sample_means$sample_means))

#calculate theoretical standard error
(sem <- 10000/(sqrt(samp_size)))

#calculate the mean of the sample standard deviation
mean_sd <- (mean(sample_sds$sample_sds))
#and use this to calculate the empirical standard error
(ese <- mean_sd/(sqrt(samp_size)))
```
#Step 4:
```{r}
n <- 1000
sample_means <- rep(NA, n) #empty vector for storing sample means
sample_sds <- rep(NA, n) #empty vector for storing sds

for (i in 1:n) {
  
  samp_size <- 5000
  samp_rate <- 1/10000
  our_sample <- rexp(n = samp_size, rate = samp_rate)

  sample_means[i] <- mean(our_sample)
  sample_sds[i] <- sd(our_sample)
}

sample_means <- tibble(sample_means)
sample_means
sample_sds <- tibble(sample_sds)
sample_sds

#Plot sample means as a histogram
sample_means %>% ggplot(aes(x = sample_means)) +
geom_histogram()

#output standard deviation of sample means
(sd(sample_means$sample_means))

#calculate theoretical standard error
sem <- 10000/(sqrt(samp_size))
sem

#calculate the mean of the sample standard deviation
mean_sd <- (mean(sample_sds$sample_sds))
#and use this to calculate the empirical standard error
ese <- mean_sd/(sqrt(samp_size))
ese

```
##Exercise 2 - Linear Regression (5 points)
```{r}
#Load train data
traindata <- read_csv('I:/train.csv')

#Fit a regression model with y = SalePrice and Features: LotArea, OverallQual, ExterQual
lin_mod1 <- lm(formula = SalePrice ~ LotArea + OverallQual + ExterQual, data = traindata)

#Use the broom package to output the coefficients and the R-squared
tidy(lin_mod1) 
glance(lin_mod1)

#Check to confirm that 'Poor' is never assigned as a value for ExterQual in this dataset, reference level in model is 'Excellent'
unique(traindata$ExterQual)
```

*Answer these questions:* <br />
**Use the broom package to output the coefficients and the R-squared** <br />
- See above code block.

**Interpret the coefficient on LotArea** <br />
- For every square foot of Lot size, the sale price increases, on average, by $1.45, with all other features being held constant.

**Interpret the coefficient on ExterQualGd** <br />
- The mean difference in the sale price between a property with 'Excellent' quality of the material on the exterior and a property with 'Good' quality of the material on the exterior is -$71,529.49, so 'Good' quality houses are less expensive than those with 'Excellent' quality material.

**Compare this model to the model we fit in HW 7 with GrLivArea, OverallQual, Neighborhood. Which
is the better fitting model?** <br />
- The adjusted R-squared for the model fit in Hw 7 was 0.78 and the adjusted R-squared for this model is 0.69, so the model we fit in Hw 7 was a better fitting model.

##Exercise 3 - AB Testing (5 points)
```{r}
#Load A/B Test data
ab_dat <- read_csv('I:/ab_test_data.csv')

#Proportion of visitors converted for each verion of the webpage
ab_dat %>%
  group_by(version, conversion) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))

#A/B Test in R
true_a <- .0415
true_b <- .1000
n_a <- 2000
n_b <- 2000

set.seed(10)
samp_a <- rbinom(n = 1, size = n_a, prob = true_a)
samp_b <- rbinom(n = 1, size = n_b, prob = true_b)

samp_a
samp_b

two_prop_test <- prop.test(c(samp_a, samp_b), c(2000, 2000))
two_prop_test$p.value

```

*Answer these questions:*  <br />
**What proportion of visitors converted for each version of the webpage?** <br />
- 4.2% of Version A visitors converted and 10% of Version B visitors converted.

**Perform the AB test in R. What is the p-value for the AB test (hypothesis test of proportions)?**  <br />
- The p-value for the AB test is  1.25388e-10. The p-value is the probability of observing a statistic as extreme, or more extreme, than what we got, if the null is true. If p-value < 0.05, we reject the null hypothesis. Therefore, we can reject the null hypothesis in this case.











