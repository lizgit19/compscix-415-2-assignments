---
title: "COMPSCIX 415.2 Homework 6"
author: "Elizabeth Eldridge"
date: "March 5, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
#install.packages("mosaicData")
library(mosaicData)

(data(Whickham))
```

##Exercise 1

Load the Whickham dataset (data(Whickham)). You will need to load the mosaicData package first, but I
also included the data as a csv file on Canvas if you would rather download it there and load it with the
readr package.

Look at the help file on this dataset to learn a bit about it. Note that the help file only exists if you are using
the dataset from the mosaicData package. If you are loading the dataset from the csv file, do a Google search
of this dataset and package name to help answer the first two questions below.

```{r}
library(mosaicData)
(data(Whickham))

?Whickham
names(Whickham)

head(Whickham, 20)
```

1. What variables are in this data set?

- The variables in the dataset are: outcome, smoker, and age.

2. How many observations are there and what does each represent?

- There are 1314 observations in this data and each represents a woman on the electoral roll in Whickham.

3. Create a table (use the R code below as a guide) and a visualization of the relationship between
smoking status and outcome, ignoring age. What do you see? Does it make sense?

- I'm seeing that using this method, the two variables in this plot will need to be recoded as numeric variables rather than character variables in order to plot in a useful way.

```{r}
Whickham %>% count( smoker , outcome )

ggplot(data = Whickham) + 
    geom_bar(aes(x = outcome, y = "Freq", fill = smoker), stat = 'identity', position = position_dodge())

#View(Whickham)
#Whickham$num_outcome <- as.numeric(Whickham$smoker)

head(Whickham$num_outcome)
###
Whickham %>% mutate(Count_outc = as.numeric(outcome))
head(Whickham)
Whickham <- Whickham %>% mutate(new_Outc = factor(outcome, levels = c(0,1))) %>%
    mutate(new_Outc2 = as.numeric(new_Outc))

#Whickham2 <- Whickham %>% recode(outcome,
#                      c('Alive' = '0', 'Dead' = '1'))

Whickham <- Whickham %>%
    mutate(age_cat = case_when(age <= 44 ~ '44 or less',
                     (age > 44 & age <= 64) ~ '44 to less than 65',
                      age > 64 ~ 'over 64'))

Whickham$age_factor <- factor(Whickham$age_cat)

class(Whickham$age_factor)
head(Whickham$age_factor)

#facet
ggplot(data = Whickham) + 
    geom_bar(aes(x = outcome, y = "Freq", fill = smoker), stat = 'identity', position = position_dodge()) + 
    facet_grid(vars(age_cat))
```

4. Recode the age variable into an ordered factor with three categories: age <= 44, age > 44 & age <=
64, and age > 64. Now, recreate visualization from above, but facet on your new age factor. What do
you see? Does it make sense?

- When the outcome variable is recoded as a numeric variable, I would expect to see that we start to see a greater outcome of 'Death' among the older age group, regardless of smoker status.


##Exercise 2

The Central Limit Theorem states that the sampling distribution of sample means is approximately Normal,
regardless of the distribution of your population. For this exercise our population distribution will be a
Gamma(1,2) distribution, and we’ll show that the sampling distribution of the mean is in fact normally
distributed.

1. Generate a random sample of size n = 10000 from a gamma(1,2) distribution and plot a histogram or
density curve. Use the code below to help you get your sample.

```{r}
library(tidyverse)
n <- 10000
# look at ?rgamma to read about this function
gamma_samp <- tibble(x = rgamma(n, shape = 1, scale = 2))

gamma_samp %>% ggplot() + 
  geom_histogram(aes(x = x), bindwidth = 10) +
  theme_bw() 
```

2. What is the mean and standard deviation of your sample? They should both be close to 2 because for
a gamma distribution:

mean = shape x scale
variance = shape x scaleˆ2

- The mean is 1.99

```{r}
mean_samp <- gamma_samp %>% .[['x']] %>% mean()
mean_samp
```

3. Pretend the distribution of our population of data looks like the plot above. Now take a sample of
size n = 30 from a Gamma(1,2) distribution, plot the histogram or density curve, and calculate the
mean and standard deviation

- Mean is 2.022 and standard deviation is 1.98

```{r}
gamma_samp %>% sample_n(30) %>% ggplot() +
  geom_histogram(aes(x = x), bindwidth = 10) +
  theme_bw()

samp1 <- gamma_samp %>% sample_n(30)

mean_samp1 <- samp1 %>% .[['x']] %>% mean()
mean_samp1 #2.022
sd_samp <- gamma_samp %>% .[['x']] %>% sd()
sd_samp   #1.98
```


4. Take a sample of size n = 30, again from the Gamma(1,2) distribution, calculate the mean, and assign
it to a vector named mean_samp. Repeat this 10000 times!!!! The code below might help.

```{r}
# create a vector with 10000 NAs
mean_samp <- rep(NA, 10000)
# start a loop
for(i in 1:10000) {
g_samp <- rgamma(30, shape = 1, scale = 2)
mean_samp[i] <- mean(g_samp)
}
# Convert vector to a tibble
mean_samp <- tibble(mean_samp)
mean_samp
```

5. Make a histogram of your collection of means from above (mean_samp).

```{r}
mean_samp %>%  ggplot() +
  geom_histogram(aes(x = mean_samp), bindwidth = 10) +
  theme_bw()
```


6. Calculate the mean and standard deviation of all of your sample means.

```{r}

#mean_Samp2 <- rep(NA, 10000)
#for(i in 1:10000) {
#mean_Samp2[i] <- mean_samp %>% sample_n(300) %>% 
#  summarize(meanvar = mean(mean_samp, na.rm = TRUE),
#                        sd_var = sd(mean_samp, na.rm = TRUE))
#                  }
#mean_samp2

##crashes my R session.

```


7. Did anything surprise you about your answers to #6?

- The answers to #6 differ a bit by observation. 

8. According to the Central Limit Theorem, the mean of your sampling distribution should be very close
to 2, and the standard deviation of your sampling distribution should be close to √σ n = √230 = 0.365.
Repeat #4-#6, but now with a sample of size n = 300 instead. Do your results match up well with the
theorem?

- The means and standard deviations seem closer to the values suggested by the Central Limit Theorem when they are based off of a larger sample size than a smaller one, same as what we see when we visualize this- a larger size will look closer to a bell curve.

